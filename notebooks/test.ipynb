{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['imputation_rules', 'deduplication_rules', 'replacement_rules'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "path = 'C:\\\\Users\\\\natha\\\\PycharmProjects\\\\gambling_premises_uk\\\\config\\\\2024_premises-licence-register_metadata.json'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='C:\\\\Users\\\\natha\\\\.cache\\\\kagglehub\\\\datasets\\\\nathanhg\\\\uk-gam-datasets\\\\versions\\\\1'\n",
    "premises = '2024_premises-licence-register.csv'\n",
    "\n",
    "# Read files into memory \n",
    "\n",
    "premises_df = pd.read_csv(os.path.join(path,premises))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check column count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns included in imputation rules\n"
     ]
    }
   ],
   "source": [
    "df = premises_df\n",
    "# log discrepencies\n",
    "columns_included = len(data['imputation_rules'].keys())\n",
    "columns_expected = len(df.columns)\n",
    "if columns_expected - columns_expected != 0:\n",
    "    print('Analyst has not included all columns in imputation rules')\n",
    "else:\n",
    "    print('All columns included in imputation rules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_standardization(df):\n",
    "    \"\"\"\n",
    "    Standardizes column names based on BigQuery lexical structure and syntax:\n",
    "    https://cloud.google.com/bigquery/docs/reference/standard-sql/lexical#column_names\n",
    "    Column names are changed if they violate the rules.\n",
    "    \n",
    "    Parameters:\n",
    "        column_names (list): The list of original column names to standardize.\n",
    "    \n",
    "    Returns:\n",
    "        list: The list of standardized column names.\n",
    "    \"\"\"\n",
    "    column_names = df.columns\n",
    "    # Define banned prefixes\n",
    "    banned_prefixes = ['_TABLE_', '_FILE_', '_PARTITION', '_ROW_TIMESTAMP', '__ROOT__', '_COLIDENTIFIER']\n",
    "    banned_prefixes = [prefix.lower() for prefix in banned_prefixes]\n",
    "\n",
    "    # Initialize storage for validated column names and duplicates\n",
    "    validated = []\n",
    "    seen = set()\n",
    "\n",
    "    for i, name in enumerate(column_names):\n",
    "        # Ensure name is a string and convert to lowercase\n",
    "        name = str(name).strip().lower()\n",
    "\n",
    "        # Replace spaces with underscores and keep valid characters only\n",
    "        name = re.sub(r'[^a-z0-9_]', '', name.replace(' ', '_'))\n",
    "\n",
    "        # Enforce max length of 300 characters\n",
    "        name = name[:300]\n",
    "\n",
    "        # Handle banned prefixes\n",
    "        for prefix in banned_prefixes:\n",
    "            if name.startswith(prefix):\n",
    "                name = name.replace(prefix, 'default', 1)\n",
    "\n",
    "        # Prepend \"column\" if the name starts with a digit or is empty\n",
    "        if not name or name[0].isdigit():\n",
    "            name = f\"column_{name}\"\n",
    "\n",
    "        # Resolve duplicates by appending a counter\n",
    "        original_name = name\n",
    "        counter = 1\n",
    "        while name in seen:\n",
    "            name = f\"{original_name}_{counter}\"\n",
    "            counter += 1\n",
    "\n",
    "        # Add the standardized name to the result and mark it as seen\n",
    "        validated.append(name)\n",
    "        seen.add(name)\n",
    "    df.columns = validated\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_and_impute_data(df, metadata):\n",
    "    \"\"\"\n",
    "    Cleans and imputes data based on provided metadata.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        metadata (dict): Metadata for cleaning, specifying column types and imputation strategies.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and imputed DataFrame.\n",
    "    \"\"\"\n",
    "    for column, rules in metadata.items():\n",
    "        col_type = rules.get(\"type\")\n",
    "        impute_method = rules.get(\"method\")\n",
    "        reference_column = rules.get(\"reference_column\")\n",
    "\n",
    "        # Handle data type conversions based on rules\n",
    "        if col_type == \"categorical\":\n",
    "            if df[column].dtype != 'object':  # Only convert if not already a string\n",
    "                df[column] = df[column].astype(str)\n",
    "                print(f'{column} converted to string')\n",
    "        elif col_type == \"numeric\":\n",
    "            if df[column].dtype != 'float64':  # Only convert if not already numeric\n",
    "                df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "                print(f'{column} converted to numeric')\n",
    "        elif col_type == \"datetime\":\n",
    "            if df[column].dtype != 'datetime64[ns]':  # Check if already datetime\n",
    "                df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "                print(f'{column} converted to datetime')\n",
    "        elif col_type == \"date\":\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df[column]):  # Check for datetime type first\n",
    "                df[column] = pd.to_datetime(df[column], format='%Y-%m-%d', errors='coerce').dt.date\n",
    "                print(f'{column} converted to date')\n",
    "\n",
    "        # Impute missing values based on specified method\n",
    "        if impute_method == \"NMAR\":\n",
    "            df.fillna({column:\"Missing\"}, inplace=True)\n",
    "            print(f'Filled nulls in {column} with \"Missing\"')\n",
    "        elif impute_method == \"MAR\":\n",
    "            if reference_column:\n",
    "                # Use the reference column to fill missing values\n",
    "                df[column] = df.groupby(reference_column)[column].transform(\n",
    "                    lambda x: x.ffill().bfill() if x.isnull().any() else x\n",
    "                )\n",
    "                print(f'Filled nulls in {column} using forward/backward fill based on {reference_column}')\n",
    "            else:\n",
    "                print(f'Skipping imputation for {column} due to missing reference_column')\n",
    "        else:\n",
    "            print(f'No imputation strategy specified for {column}')\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_standardization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['address_line_2'].isna()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account_number converted to numeric\n",
      "Filled nulls in account_number using forward/backward fill based on account_name\n",
      "Filled nulls in account_name using forward/backward fill based on account_number\n",
      "Filled nulls in premises_activity with \"Missing\"\n",
      "Filled nulls in local_authority with \"Missing\"\n",
      "Filled nulls in address_line_1 with \"Missing\"\n",
      "Filled nulls in address_line_2 with \"Missing\"\n",
      "Filled nulls in city using forward/backward fill based on account_number\n",
      "Filled nulls in postcode using forward/backward fill based on account_number\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_and_impute_data(df,data['imputation_rules'])\n",
    "df[df['address_line_2'].isna()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deduplicate': True}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['deduplication_rules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2660"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate(df, metadata):\n",
    "    \"\"\"\n",
    "    Handles deduplication for the entire dataset based on metadata.\n",
    "    Checks for duplicates before attempting to drop any.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        metadata (dict): Metadata defining deduplication rules.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame after applying deduplication rules.\n",
    "    \"\"\"\n",
    "    deduplication_required = metadata.get(\"deduplication_rules\", {}).get(\"deduplicate\", False)\n",
    "    if deduplication_required:\n",
    "        # Check if there are any duplicate rows in the DataFrame\n",
    "        duplicate_count = df.duplicated().sum()\n",
    "        \n",
    "        if duplicate_count > 0:\n",
    "            # If duplicates are found, drop them\n",
    "            df = df.drop_duplicates(keep=\"first\")\n",
    "            print(f\"Dropped {duplicate_count} duplicate rows.\")\n",
    "    else:\n",
    "        print(\"No duplicates to drop.\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates to drop.\n"
     ]
    }
   ],
   "source": [
    "deduplicate(df,data['deduplication_rules'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premises_activity': {'Casino 2005': 'Casino'},\n",
       " 'address_line_1': {\"caesar's palace\": 'caesars palace',\n",
       "  'boylesport': 'boylesports',\n",
       "  'coastal amusements limited': 'coastal amusements',\n",
       "  'carousel amusements limited': 'carousel amusements',\n",
       "  'blue anchor leisure limited': 'blue anchor leisure',\n",
       "  'betting shop operations limited': 'betting shop operations',\n",
       "  'betextra': 'bet extra',\n",
       "  'betszone': 'betzone',\n",
       "  'a & s leisure group limited': 'a & s leisure group',\n",
       "  'a & s leisure': 'a & s leisure group',\n",
       "  'cashino gaming limited': 'cashino gaming'},\n",
       " 'address_line_2': {'coral island, unit 2 promenade': 'coral island, unit 2, promenade',\n",
       "  '98a -99 high street, gorleston': '98a-99 high street, gorleston',\n",
       "  '36 -42 marine terrace ': '36-42 marine terrace'},\n",
       " 'city': {'Middlesborough': 'Middlesbrough',\n",
       "  'Burton-on-trent': 'Burton-upon-trent',\n",
       "  'Blaydon-upon-tyne': 'Blaydon-on-tyne',\n",
       "  'Bury': 'Bury st. edmunds',\n",
       "  'Lytham': 'Lytham st. annes',\n",
       "  'Richmond': 'Richmond-upon-thames'}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['replacement_rules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_strings(df, metadata):\n",
    "   \n",
    "    for column, replacements in metadata.items():\n",
    "        if column not in df.columns:\n",
    "            continue\n",
    "        df[column] = df[column].astype(str).apply(lambda x: x.strip().lower())\n",
    "        # Apply replacement rules and print details\n",
    "        for old_value, new_value in replacements.items():\n",
    "            old_value = old_value.strip().lower()\n",
    "            new_value = new_value.strip().lower()\n",
    "            occurrences = (df[column] == old_value).sum()\n",
    "            if occurrences > 0:\n",
    "                print(f'{column}: Replacing {occurrences} occurrences of \"{old_value}\" with \"{new_value.title()}\".')\n",
    "                df[column] = df[column].replace({old_value.lower(): new_value.title()}, regex=True)\n",
    "            else: \n",
    "                print(f'{column} has {occurrences} occurances of {old_value.lower()}')\n",
    "        # Confirm number of replacements\n",
    "    print('Completed all replacements.')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premises_activity: Replacing 1 occurrences of \"casino 2005\" with \"Casino\".\n",
      "address_line_1: Replacing 1 occurrences of \"caesar's palace\" with \"Caesars Palace\".\n",
      "address_line_1: Replacing 1 occurrences of \"boylesport\" with \"Boylesports\".\n",
      "address_line_1: Replacing 10 occurrences of \"coastal amusements limited\" with \"Coastal Amusements\".\n",
      "address_line_1: Replacing 1 occurrences of \"carousel amusements limited\" with \"Carousel Amusements\".\n",
      "address_line_1: Replacing 1 occurrences of \"blue anchor leisure limited\" with \"Blue Anchor Leisure\".\n",
      "address_line_1: Replacing 9 occurrences of \"betting shop operations limited\" with \"Betting Shop Operations\".\n",
      "address_line_1: Replacing 1 occurrences of \"betextra\" with \"Bet Extra\".\n",
      "address_line_1: Replacing 1 occurrences of \"betszone\" with \"Betzone\".\n",
      "address_line_1: Replacing 1 occurrences of \"a & s leisure group limited\" with \"A & S Leisure Group\".\n",
      "address_line_1: Replacing 4 occurrences of \"a & s leisure\" with \"A & S Leisure Group\".\n",
      "address_line_1: Replacing 90 occurrences of \"cashino gaming limited\" with \"Cashino Gaming\".\n",
      "address_line_2: Replacing 1 occurrences of \"coral island, unit 2 promenade\" with \"Coral Island, Unit 2, Promenade\".\n",
      "address_line_2: Replacing 1 occurrences of \"98a -99 high street, gorleston\" with \"98A-99 High Street, Gorleston\".\n",
      "address_line_2: Replacing 1 occurrences of \"36 -42 marine terrace\" with \"36-42 Marine Terrace\".\n",
      "city: Replacing 1 occurrences of \"middlesborough\" with \"Middlesbrough\".\n",
      "city: Replacing 1 occurrences of \"burton-on-trent\" with \"Burton-Upon-Trent\".\n",
      "city: Replacing 1 occurrences of \"blaydon-upon-tyne\" with \"Blaydon-On-Tyne\".\n",
      "city: Replacing 9 occurrences of \"bury\" with \"Bury St. Edmunds\".\n",
      "city: Replacing 1 occurrences of \"lytham\" with \"Lytham St. Annes\".\n",
      "city: Replacing 1 occurrences of \"richmond\" with \"Richmond-Upon-Thames\".\n",
      "Completed all replacements.\n"
     ]
    }
   ],
   "source": [
    "clean_strings(df,data['replacement_rules'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env_folder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
